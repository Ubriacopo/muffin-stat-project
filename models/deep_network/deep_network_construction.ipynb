{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-27T21:12:40.291527Z",
     "start_time": "2024-05-27T21:12:40.289637Z"
    }
   },
   "source": "default_values = {\"batch_size\": 32, \"epochs\": 15, \"learning_rate\": 1e-3}",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T16:17:32.907385Z",
     "start_time": "2024-05-22T16:17:32.905429Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "We want start by creating a custom simple DNN.\n",
    "\n",
    "# 1 - Load data\n",
    "Total splitting of data will be [64%, 16%, 20%] (If we consider test and train to be the full set) <br />\n",
    "Best practices suggest to go for a [70%, 15%, 15%] splitting but we will just keep it this way."
   ],
   "id": "6a7914d43ec39f64"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:12:41.257761Z",
     "start_time": "2024-05-27T21:12:41.243874Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import models.structure.base_model_wrapper\n",
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import dataset_loader, dataset_information\n",
    "\n",
    "train, test = dataset_loader((224, 224))\n",
    "# todo rewrite some of it to be consistent\n",
    "# We take 20% of train as validation. \n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=default_values[\"batch_size\"], shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=default_values[\"batch_size\"], shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test, batch_size=default_values[\"batch_size\"], shuffle=True)"
   ],
   "id": "93d280da62d3480f",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:13:45.886706Z",
     "start_time": "2024-05-27T21:13:10.899634Z"
    }
   },
   "cell_type": "code",
   "source": [
    "mean, variance = dataset_information(local_train, (224, 224))\n",
    "measures = {\"mean\": mean, \"variance\": variance}"
   ],
   "id": "64e2f849f20b854",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T20:49:08.370188Z",
     "start_time": "2024-05-22T20:49:08.365611Z"
    }
   },
   "cell_type": "code",
   "source": "measures",
   "id": "d5187bf5c39444ce",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T16:15:27.200069Z",
     "start_time": "2024-05-22T16:14:54.358204Z"
    }
   },
   "cell_type": "markdown",
   "source": [
    "# 2 - First model\n",
    "Our first model is a simple CNN. <br><br />\n",
    "\n",
    "\n",
    "## 2.1 - Model definition "
   ],
   "id": "f6c4f7d66f88828d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T20:49:30.519148Z",
     "start_time": "2024-05-22T20:49:30.517202Z"
    }
   },
   "cell_type": "code",
   "source": "project_definition: dict[str, any] = {\"name\": \"hand_tailored_v1\"}",
   "id": "3ae85a143a39d3d5",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T20:49:31.766434Z",
     "start_time": "2024-05-22T20:49:31.760918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.augmentation_wrapper import NormalizedModelWrapper\n",
    "from keras.layers import Conv2D, MaxPool2D, Input, Flatten, Dense, Layer\n",
    "\n",
    "\n",
    "class HandTailoredDeepNet(NormalizedModelWrapper):\n",
    "    def make_layers(self, input_shape: (int, int, int)) -> tuple[Layer, Layer]:\n",
    "        input_layer = Input(shape=input_shape, name=self.__class__.__name__)\n",
    "        x = Flatten(data_format=self.data_format.value)(input_layer)\n",
    "\n",
    "        # The number I chose are arbitrary\n",
    "        x = Dense(units=1024, activation='relu')(x)\n",
    "        x = Dense(units=256, activation=\"relu\")(x)\n",
    "        output_layer = Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        return input_layer, output_layer"
   ],
   "id": "cf2b6261d102fb74",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 - Model instance and learning",
   "id": "fe5601e518d4b054"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T20:49:41.379559Z",
     "start_time": "2024-05-22T20:49:41.279769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.learning_parameters.sgd_learning_parameters import SgdLearningParameters\n",
    "\n",
    "model_generator = HandTailoredDeepNet()\n",
    "model_generator.load_dataset_mean_and_variance(measures[\"mean\"], measures[\"variance\"])\n",
    "\n",
    "model = model_generator.make_model((3, 224, 224))\n",
    "# Default Keras learning-rate Value (0.01) doesnt work. \n",
    "# We always have a huge loss therefore we decrease it.\n",
    "SgdLearningParameters(learning_rate=1e-3).compile_model(model)\n",
    "\n",
    "model.summary()"
   ],
   "id": "7c0e45bc3d96f09b",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:00:59.428269Z",
     "start_time": "2024-05-22T20:49:46.059848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "# We fix the number of epochs for now. Later we will add early stopping.\n",
    "model.fit(x=train_dataloader, validation_data=validation_dataloader, epochs=default_values[\"epochs\"], callbacks=[\n",
    "    # To persist the history\n",
    "    keras.callbacks.CSVLogger(f\"{project_definition['name']}_train.csv\", separator=\",\", append=True)\n",
    "])\n",
    "\n",
    "persist_model: bool = True\n",
    "if persist_model:\n",
    "    model.save(f'{project_definition[\"name\"]}.keras')"
   ],
   "id": "312a95545f20a9b4",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:01:11.138245Z",
     "start_time": "2024-05-22T21:00:59.429185Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")"
   ],
   "id": "9321075a97a1d9ce",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 - Results summary",
   "id": "5df85a247d3c5c27"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:01:11.537305Z",
     "start_time": "2024-05-22T21:01:11.138884Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_processing import make_loss_graphs, make_loss_accuracy_graphs, add_tuner_iteration_to_data\n",
    "import pandas\n",
    "\n",
    "csv = pandas.read_csv(f\"{project_definition['name']}_train.csv\")\n",
    "add_tuner_iteration_to_data(csv)\n",
    "\n",
    "loss_graph = make_loss_graphs(csv)\n",
    "acc_graph = make_loss_accuracy_graphs(csv)\n",
    "\n",
    "loss_graph.update_layout(title=\"Loss vs Val_loss in tuner search per epoch (Val dashed)\").show()\n",
    "acc_graph.update_layout(title=\"Accuracy vs Val_Accuracy in tuner search per epoch (Val dashed)\").show()"
   ],
   "id": "70dbfcf8c1796126",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We are very clearly overfitting. We might reduce the model size to better generalize the data as it is insanely huge right now.",
   "id": "e43593312fbf890"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Considering the previous results it would be interesting to see if it is possible to make a smaller model able to generalize the function.\n",
    "# 3 - Smaller model"
   ],
   "id": "fd06e643caac1b65"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.1 - Model definition\n",
   "id": "d70c30aedc0542fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "project_definition: dict[str, any] = {\"name\": \"hand_tailored_small\"}",
   "id": "cc228b699d13b6e8",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from keras.layers import Conv2D, MaxPool2D, Input, Flatten, Dense, Layer\n",
    "\n",
    "\n",
    "class SmallHandTailoredDeepNet(NormalizedModelWrapper):\n",
    "    def make_layers(self, input_shape: (int, int, int)) -> tuple[Layer, Layer]:\n",
    "        input_layer = Input(shape=input_shape, name=self.__class__.__name__)\n",
    "        x = Flatten(data_format=self.data_format.value)(input_layer)\n",
    "\n",
    "        # The number I chose are arbitrary\n",
    "        x = Dense(units=512, activation='relu')(x)\n",
    "        x = Dense(units=64, activation=\"relu\")(x)\n",
    "\n",
    "        output_layer = Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        return input_layer, output_layer"
   ],
   "id": "9e40685345ea3748",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.2 - Model instance and learning",
   "id": "305525f40a5c3b4d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:01:58.868617Z",
     "start_time": "2024-05-22T21:01:58.843830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.learning_parameters.sgd_learning_parameters import SgdLearningParameters\n",
    "\n",
    "model_generator = SmallHandTailoredDeepNet()\n",
    "model_generator.load_dataset_mean_and_variance(mean, variance)\n",
    "\n",
    "model = model_generator.make_model((3, 224, 224))\n",
    "# Default Keras learning-rate Value (0.01) doesnt work. \n",
    "# We always have a huge loss therefore we decrease it.\n",
    "SgdLearningParameters(learning_rate=1e-3).compile_model(model)\n",
    "\n",
    "model.summary()"
   ],
   "id": "b716757bf9466b39",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:12:42.716630Z",
     "start_time": "2024-05-22T21:02:01.739623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "# We fix the number of epochs for now. Later we will add early stopping.\n",
    "model.fit(x=train_dataloader, validation_data=validation_dataloader, epochs=default_values[\"epochs\"], callbacks=[\n",
    "    # To persist the history\n",
    "    keras.callbacks.CSVLogger(f\"{project_definition['name']}_train.csv\", separator=\",\", append=True)\n",
    "])\n",
    "\n",
    "persist_model: bool = True\n",
    "if persist_model:\n",
    "    model.save(f'{project_definition[\"name\"]}.keras')"
   ],
   "id": "f1e307a4588d2fd7",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:15:27.459255Z",
     "start_time": "2024-05-22T21:15:15.792076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")"
   ],
   "id": "c61f5f391a355ee1",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3.3 - Results summary",
   "id": "4834cf3cd4c6614"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:15:33.246944Z",
     "start_time": "2024-05-22T21:15:33.140700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_processing import make_loss_graphs, make_loss_accuracy_graphs, add_tuner_iteration_to_data\n",
    "import pandas\n",
    "\n",
    "csv = pandas.read_csv(f\"{project_definition['name']}_train.csv\")\n",
    "add_tuner_iteration_to_data(csv)\n",
    "\n",
    "loss_graph = make_loss_graphs(csv)\n",
    "acc_graph = make_loss_accuracy_graphs(csv)\n",
    "\n",
    "loss_graph.update_layout(title=\"Loss vs Val_loss in tuner search per epoch (Val dashed)\").show()\n",
    "acc_graph.update_layout(title=\"Accuracy vs Val_Accuracy in tuner search per epoch (Val dashed)\").show()"
   ],
   "id": "e1ebd55d7f628922",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The model is still hardly overfitting",
   "id": "7ea5f3a573e53adb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4 - Even smaller model\n",
    "\n",
    "## 4.1 - Model definition"
   ],
   "id": "e7c2820e20dc0696"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:12:26.460121Z",
     "start_time": "2024-05-27T21:12:26.458515Z"
    }
   },
   "cell_type": "code",
   "source": "project_definition: dict[str, any] = {\"name\": \"hand_tailored_xs\"}",
   "id": "2f18b9aeca10618d",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:13:00.601162Z",
     "start_time": "2024-05-27T21:13:00.598388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.augmentation_wrapper import NormalizedModelWrapper\n",
    "from keras.layers import Conv2D, MaxPool2D, Input, Flatten, Dense, Layer\n",
    "\n",
    "\n",
    "class VerySmallHandTailoredDeepNet(NormalizedModelWrapper):\n",
    "    def make_layers(self, input_shape: (int, int, int)) -> tuple[Layer, Layer]:\n",
    "        input_layer = Input(shape=input_shape, name=self.__class__.__name__)\n",
    "        x = Flatten(data_format=self.data_format.value)(input_layer)\n",
    "\n",
    "        # The number I chose are arbitrary\n",
    "        x = Dense(units=128, activation='relu')(x)\n",
    "        output_layer = Dense(units=1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        return input_layer, output_layer"
   ],
   "id": "943e0e6a08b26f09",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4.2 - Model instance and learning",
   "id": "5d1c76c49cce85c4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:13:45.984779Z",
     "start_time": "2024-05-27T21:13:45.887551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.learning_parameters.sgd_learning_parameters import SgdLearningParameters\n",
    "\n",
    "model_generator = VerySmallHandTailoredDeepNet()\n",
    "model_generator.load_dataset_mean_and_variance(mean, variance)\n",
    "\n",
    "model = model_generator.make_model((3, 224, 224))\n",
    "# Default Keras learning-rate Value (0.01) doesnt work. \n",
    "# We always have a huge loss therefore we decrease it.\n",
    "SgdLearningParameters(learning_rate=1e-4).compile_model(model)\n",
    "\n",
    "model.summary()"
   ],
   "id": "83109bda547e59d1",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-27T21:19:27.752848Z",
     "start_time": "2024-05-27T21:13:45.985423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "# We fix the number of epochs for now. Later we will add early stopping.\n",
    "model.fit(x=train_dataloader, validation_data=validation_dataloader, epochs=default_values[\"epochs\"], callbacks=[\n",
    "    # To persist the history\n",
    "    keras.callbacks.CSVLogger(f\"{project_definition['name']}_train.csv\", separator=\",\", append=True)\n",
    "])\n",
    "\n",
    "persist_model: bool = True\n",
    "if persist_model:\n",
    "    model.save(f'{project_definition[\"name\"]}.keras')"
   ],
   "id": "bcf7733a80204e47",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:26:47.563892Z",
     "start_time": "2024-05-22T21:26:36.335103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")"
   ],
   "id": "f3c3ec29ec759820",
   "execution_count": 26,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-22T21:28:39.888483Z",
     "start_time": "2024-05-22T21:28:39.782028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_processing import make_loss_graphs, make_loss_accuracy_graphs, add_tuner_iteration_to_data\n",
    "import pandas\n",
    "\n",
    "csv = pandas.read_csv(f\"{project_definition['name']}_train.csv\")\n",
    "add_tuner_iteration_to_data(csv)\n",
    "\n",
    "loss_graph = make_loss_graphs(csv)\n",
    "acc_graph = make_loss_accuracy_graphs(csv)\n",
    "\n",
    "loss_graph.update_layout(title=\"Loss vs Val_loss in tuner search per epoch (Val dashed)\").show()\n",
    "acc_graph.update_layout(title=\"Accuracy vs Val_Accuracy in tuner search per epoch (Val dashed)\").show()"
   ],
   "id": "80ede1dc924cad77",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "# https://www.tensorflow.org/tutorials/keras/overfit_and_underfit#strategies_to_prevent_overfitting",
   "id": "a67e923f765dca89",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
