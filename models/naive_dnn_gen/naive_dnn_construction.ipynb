{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-23T13:51:16.238034Z",
     "start_time": "2024-04-23T13:51:14.256845Z"
    }
   },
   "source": [
    "import os\n",
    "\n",
    "# Why Torch? You'll find the answer in the .md files! \n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import keras\n",
    "\n",
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "import keras_tuner\n",
    "from torch.utils.data import DataLoader\n",
    "from models.naive_dnn_gen.naive_dnn import NaiveDnnTunableWrapper\n",
    "from models.structure.tunable_model_family_hypermodel import TunableModelFamilyHypermodel\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:08:31.375511Z",
     "start_time": "2024-04-23T16:08:31.362170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initial steps\n",
    "hyperparameters = keras_tuner.HyperParameters()\n",
    "train, test = dataset_loader((128, 128), is_grayscale=False)\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=16, shuffle=True)"
   ],
   "id": "6a34684b0cf5e5c9",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# First Space Search\n",
    "We expect the process to not yield good results as the DNN requires a huge number of neurons to work properly. \n",
    "\n",
    "After analizing the results we may consider a second search that is restricted to a smaller set of possible parameters"
   ],
   "id": "7b9234f72b79c47f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T17:05:22.686803Z",
     "start_time": "2024-04-23T17:05:22.514776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "from utils.my_tuner import HistoryDeletingBayesianOptimization\n",
    "\n",
    "hypermodel = TunableModelFamilyHypermodel((3, 128, 128), NaiveDnnTunableWrapper())\n",
    "\n",
    "# Our model uses SGD\n",
    "hyperparameters.Fixed(\"lr\", 1e-4)\n",
    "hyperparameters.Fixed(\"momentum\", 0.9)\n",
    "\n",
    "hyperparameters.Fixed(\"layers\", 3)\n",
    "# I expect the tuner to find better models without dropout as dropout learning takes usually empircally longer.\n",
    "# We hope to find at least one structure\n",
    "tuner = HistoryDeletingBayesianOptimization(\n",
    "    hypermodel,\n",
    "    hyperparameters=hyperparameters,\n",
    "    objective='val_loss',\n",
    "    tune_new_entries=True,\n",
    "    overwrite=False,\n",
    "    directory=\"dnn-search\",\n",
    "    max_trials=15,\n",
    "    project_name=\"three-layers\"\n",
    ")"
   ],
   "id": "54357f384f26fecd",
   "execution_count": 40,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# todo override on_epoch_end to write info of it\n",
    "tuner.search(train_dataloader, epochs=22, validation_data=validation_dataloader,\n",
    "             callbacks=[keras.callbacks.CSVLogger(\"dnn-search/two-layers/search.log\", separator=\",\", append=True)])"
   ],
   "id": "27f25fd300ffb60f",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T15:51:13.765085Z",
     "start_time": "2024-04-24T15:51:13.761970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tuner.results_summary(5)  # Top 5\n",
    "tuner.get_best_hyperparameters(5)\n"
   ],
   "id": "a01ca451c64bce38",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:43:39.537305Z",
     "start_time": "2024-04-23T16:11:20.847873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The top 5 behave kinda bad actually but let's try to train them via Early Stopping for a longer time and see how good they generalize.\n",
    "# We try with a model that has dropout which typically requires more time to converge\n",
    "train_model = NaiveDnnTunableWrapper()\n",
    "train_model.load_parameters(tuner.get_best_hyperparameters(5)[3])\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=test, batch_size=16, shuffle=True)\n",
    "\n",
    "fine_tune_model = train_model.make_model((3, 128, 128))\n",
    "train_model.compile_model(fine_tune_model, keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9, nesterov=True))\n",
    "\n",
    "model_history = fine_tune_model.fit(\n",
    "    train_dataloader, validation_data=validation_dataloader, epochs=100, callbacks=[\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='min')\n",
    "    ]\n",
    ")\n"
   ],
   "id": "486eb9a7a160106c",
   "execution_count": 31,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T17:01:48.954011Z",
     "start_time": "2024-04-23T17:01:48.789038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas\n",
    "metrics_df1 = pandas.DataFrame(model_history.history)\n",
    "metrics_df1[[\"loss\", \"val_loss\"]].plot(ylim=(0, 1))\n",
    "metrics_df1[[\"accuracy\", \"val_accuracy\"]].plot(ylim=(0, 1))"
   ],
   "id": "b4a98b62b9440192",
   "execution_count": 38,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T17:01:52.953380Z",
     "start_time": "2024-04-23T17:01:52.947626Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df1",
   "id": "93b671d1dc075a50",
   "execution_count": 39,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b967c14a45f58fe1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f7afcfde660138fc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Considerations\n",
    "The results make me believe that the network has a hard time generalizing the data. The learning process might be too slow to see if the parameters we found are good enough. We should try to tune the lr of SGD. \n",
    "\n",
    "I will either way try to train the 3 best networks to see if (via early stopping) the convergence is met with a good aproximation.\n",
    "What I expect to see is that (maybe outisde the first one that will do a little better) all the networks will yeild a low accuracy on train and test (around 80%/70%).\n",
    "\n",
    "To try this we use K-Fold Cross Validation"
   ],
   "id": "a453c09e904b8092"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4a74a0a9b44b1d59"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.k_fold_cv_procedure import k_fold_cv_procedure\n",
    "from models.naive_dnn_gen.naive_dnn import HiddenLayerStructure, NaiveDnnWrapper\n",
    "\n",
    "train, _ = dataset_loader((128, 128), is_grayscale=False)\n",
    "k_fold_controller = KFoldDatasetWrapper(5)\n",
    "k_fold_controller.load_data(train)\n",
    "\n",
    "model_generator = NaiveDnnWrapper()\n",
    "model_generator.hidden_layers = [\n",
    "    HiddenLayerStructure(2048, None),\n",
    "    HiddenLayerStructure(1024, None),\n",
    "]\n",
    "\n",
    "history = k_fold_cv_procedure(model_generator, (3, 128, 128), 'SGD', k_fold_controller)"
   ],
   "id": "5108aaee617f77d9",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-21T12:46:21.369184Z",
     "start_time": "2024-04-21T11:20:27.187132Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "from models.naive_dnn_gen.naive_dnn import HiddenLayerStructure, NaiveDnnWrapper\n",
    "\n",
    "train, test = dataset_loader((128, 128), is_grayscale=False)\n",
    "\n",
    "k_fold_controller = KFoldController(5)\n",
    "k_fold_controller.load_data(train)\n",
    "\n",
    "local_train, validation = k_fold_controller.get_data_for_fold(0)\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=16, shuffle=True)\n",
    "\n",
    "model_generator = NaiveDnnWrapper()\n",
    "model_generator.hidden_layers = [\n",
    "    HiddenLayerStructure(2048, None),\n",
    "    HiddenLayerStructure(1024, None),\n",
    "]\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(k_fold_controller.k):\n",
    "    # Release memory to avoid OOM during tuning.\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "    train_i, validation_i = k_fold_controller.get_data_for_fold(i)\n",
    "\n",
    "    train_dataloader = DataLoader(dataset=train_i, batch_size=16, shuffle=True)\n",
    "    validation_dataloader = DataLoader(dataset=validation_i, batch_size=16, shuffle=True)\n",
    "\n",
    "    model_i = model_generator.make_model((3, 128, 128))\n",
    "    model_generator.compile_model(model_i, 'SGD')\n",
    "    i_history = model_i.fit(\n",
    "        train_dataloader, validation_data=validation_dataloader, epochs=100,\n",
    "        callbacks=[\n",
    "            keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='min')]\n",
    "    )\n",
    "\n",
    "    history.append(i_history)\n"
   ],
   "id": "f80ae010647013e9",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T18:00:34.853763Z",
     "start_time": "2024-04-22T18:00:34.849816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "expected_val_loss = np.sum([history[i].history['val_loss'][-1] for i in range(len(history))]) / len(history)\n",
    "expected_val_acc = np.sum([history[i].history['val_accuracy'][-1] for i in range(len(history))]) / len(history)\n",
    "\n",
    "# Via early stopping and parameter restoring\n",
    "min_val_loss_i = [np.argmin(history[i].history['val_loss']) for i in range(len(history))]\n",
    "best_validation_loss_values = [history[index].history['val_loss'][item] for index, item in enumerate(min_val_loss_i)]\n",
    "best_validation_acc_values = [history[index].history['val_accuracy'][item] for index, item in enumerate(min_val_loss_i)]\n",
    "best_expected_val_loss = np.sum(best_validation_loss_values) / len(min_val_loss_i)\n",
    "best_expected_val_accuracy = np.sum(best_validation_acc_values) / len(min_val_loss_i)\n",
    "\n",
    "print(\n",
    "    f\"We expect the model to perform with a {expected_val_loss} loss and a {expected_val_acc} accuracy by normal training. \\n\"\n",
    "    f\"If we consider to restore the best weights we expect a model to perform with: {best_expected_val_loss} loss and {best_expected_val_accuracy} accuracy\")\n",
    "\n",
    "# The model under performs and slowly converges for training but can't generalize for the validation data. It seems like we are underfitting.\n",
    "# Patience might be set too low. We higher it to 10 for the next test as suggested: https://stats.stackexchange.com/questions/231061/how-to-use-early-stopping-properly-for-training-deep-neural-network\n",
    "# http://users.diag.uniroma1.it/~palagi/didattica/sites/default/files/allegati/OMML_12th_lect_19-20_early%20stopping.pdf"
   ],
   "id": "b77f90f09496ebbd",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T18:01:37.050846Z",
     "start_time": "2024-04-22T18:01:36.579282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics_df1 = pandas.DataFrame(history[0].history)\n",
    "metrics_df1[[\"loss\", \"val_loss\"]].plot(ylim=(0, 1))\n",
    "metrics_df1[[\"accuracy\", \"val_accuracy\"]].plot(ylim=(0, 1))\n",
    "metrics_df1[[\"loss\", \"val_loss\"]].plot()\n",
    "metrics_df1[[\"accuracy\", \"val_accuracy\"]].plot()"
   ],
   "id": "99ad89177e1f3dca",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T18:33:13.054210Z",
     "start_time": "2024-04-22T18:33:13.046581Z"
    }
   },
   "cell_type": "code",
   "source": "metrics_df1",
   "id": "1e9a43721718bfaf",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T18:19:03.220868Z",
     "start_time": "2024-04-22T18:02:34.523466Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataloader = DataLoader(dataset=train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=test, batch_size=16, shuffle=True)\n",
    "\n",
    "model_generator = NaiveDnnWrapper()\n",
    "model_generator.hidden_layers = [\n",
    "    HiddenLayerStructure(550, None),\n",
    "    HiddenLayerStructure(550, None),\n",
    "]\n",
    "\n",
    "end_model = model_generator.make_model((3, 128, 128))\n",
    "model_generator.compile_model(end_model, 'SGD')\n",
    "history = end_model.fit(train_dataloader, validation_data=validation_dataloader, epochs=100,\n",
    "                        callbacks=[keras.callbacks.EarlyStopping(\n",
    "                            monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='min'\n",
    "                        )])"
   ],
   "id": "d06ce3c92dff3ee",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T18:32:41.767499Z",
     "start_time": "2024-04-22T18:32:41.765006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "best_trial_index = np.argmin(history.history['val_loss'])\n",
    "history.history['val_loss'][best_trial_index]\n",
    "\n",
    "print(\n",
    "    f\"We expect the model to perform with a {best_expected_val_loss} loss and a {best_expected_val_accuracy} accuracy with early stopping and observed. \\n\"\n",
    "    f\"a real performance on the full set of with: {history.history['val_loss'][best_trial_index]} loss and {history.history['val_accuracy'][best_trial_index]} accuracy\"\n",
    ")\n",
    "\n",
    "# Our model is statistically coherent"
   ],
   "id": "c16068d7ecc27b25",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "6c3feb37d2140f96",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "e07a17caa6e0cfe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-23T16:01:14.448222Z",
     "start_time": "2024-04-23T16:01:14.441900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "a = 0\n",
    "tuner.results_summary()  # Ten best\n",
    "tuner.get_best_hyperparameters(5)"
   ],
   "id": "78f85d037223d93b",
   "execution_count": 16,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
