{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T10:03:24.952304Z",
     "start_time": "2024-05-09T10:03:23.372853Z"
    }
   },
   "source": [
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "\n",
    "# What image size? Let's go for a common image size while allowing the network to be smaller:\n",
    "# 160x160: A bit larger than 128x128, this size provides slightly more detail while still maintaining reasonable computational requirements. It's suitable for various tasks, including object detection and classification.\n",
    "train, test = dataset_loader((160, 160), is_grayscale=False)\n",
    "\n",
    "# We take 20% of dataset as validation.\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test, batch_size=32, shuffle=True)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T10:03:24.955304Z",
     "start_time": "2024-05-09T10:03:24.953086Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Validation set is {len(validation) / (len(train) + len(test)) * 100:.2f}% of total dataset.\")",
   "id": "886328b8aac9ef1c",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T10:03:25.914464Z",
     "start_time": "2024-05-09T10:03:24.956030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.learning_parameters.sgd_learning_parameters import SgdLearningParameters\n",
    "from models.naive_dnn_gen.naive_dnn import NaiveDnnWrapper\n",
    "\n",
    "# For input of size 160x160x3 (150528) we have the problem to choose the neurons between that and 1 (output).\n",
    "# It would be reasonable to have large hidden layers. Yet we are limited.\n",
    "model = NaiveDnnWrapper().make_model((3, 160, 160))\n",
    "\n",
    "learning_parameters = SgdLearningParameters(learning_rate=1e-4)\n",
    "learning_parameters.compile_model(model)\n",
    "\n",
    "model.summary()"
   ],
   "id": "275c21fcce45d8c8",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I don't know why but it considers 16 for batch size while I didn't explicit it.",
   "id": "33559fb682d66330"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T07:53:18.200288Z",
     "start_time": "2024-05-09T07:53:18.094493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "keras.utils.plot_model(\n",
    "    model, to_file='hand_tailored_conv_net.png', show_layer_names=True, expand_nested=True, show_shapes=True\n",
    ")"
   ],
   "id": "5f1d5432ec31b95a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:20:41.399260Z",
     "start_time": "2024-05-09T07:53:18.201161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "# In this simple approach we have fixed [Train 0.8 - Val 0.2 - Test] without doing any k-fold CV.\n",
    "# The number of epoches is fixed by early stopping. (In the case of k-fold-CV it will be a hp given by average performance?)\n",
    "history = model.fit(x=train_dataloader, validation_data=validation_dataloader, batch_size=32, epochs=120, callbacks=[\n",
    "    # To avoid going further when training\n",
    "    keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', min_delta=1e-4, patience=10,\n",
    "        verbose=1, mode='min', restore_best_weights=True\n",
    "    ),\n",
    "    # To persist the history\n",
    "    keras.callbacks.CSVLogger(f\"hand-tailored-dnn.log\", separator=\",\", append=True)\n",
    "])"
   ],
   "id": "569773d51ded19b7",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:21:17.863961Z",
     "start_time": "2024-05-09T08:21:01.349641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "persist_model: bool = True\n",
    "\n",
    "# We don't want to persist the model as I already saved it.\n",
    "if persist_model:\n",
    "    model.save('hand_tailored_dnn_net.keras')"
   ],
   "id": "49870fd41fc78e2a",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:21:21.140532Z",
     "start_time": "2024-05-09T08:21:21.138285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")\n",
    "print(f\"The model is not that bad considering we have no pre-processing done and the parameters were chosen freely\")"
   ],
   "id": "5c943ad153fa2dac",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:21:29.792898Z",
     "start_time": "2024-05-09T08:21:29.789304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas\n",
    "\n",
    "csv = pandas.read_csv(f\"hand-tailored-dnn.log\")"
   ],
   "id": "3cf12415eb94d26a",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:22:20.484458Z",
     "start_time": "2024-05-09T08:22:20.270813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "loss_figure = px.line(csv, x=\"epoch\", y=[\"loss\", \"val_loss\"], template=\"plotly_white\", markers=True)\n",
    "loss_figure.update_layout(title=\"Loss\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")"
   ],
   "id": "8b21900c81c3fef0",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:22:15.947285Z",
     "start_time": "2024-05-09T08:22:15.919696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure = px.line(csv, x=\"epoch\", y=[\"accuracy\", \"val_accuracy\"], template=\"plotly_white\", markers=True)\n",
    "figure.update_layout(title=\"Accuracy\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")"
   ],
   "id": "952e4de490c66fd2",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "e5271e69025e2bfd",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model isn't necessarily evil. It works fair enough not being tuned.\n",
    "Regularization and images pre-processing could lead to some good results while maybe it is the case to increase the parameters? All these considerations are to left to thought as the CNN (which is a state of the art approach) is only 1/3 of the current network and performs consistently better in the same amount of epochs on test while having more trouble fitting the training set.\n",
    "\n",
    "### CNN seem to be the path to take (as tweaking a DNN can be a really hard task)"
   ],
   "id": "1c5087a0fe9d576b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "44fee2d65b2a53d8",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
