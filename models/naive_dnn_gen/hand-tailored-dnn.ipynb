{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "First idea of a NN for the task of image classification.",
   "id": "3bb3f3a43a394865"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 - Load the data\n",
    "Load the data from the downloaded dataset. As we are working on a more classical DNN (made of various Dense layers) the number of parameters can increase greatly based on the input size. For this reason we go for a smaller yet common image size: 160 x 160."
   ],
   "id": "87e58e84ed39c386"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-09T10:03:24.952304Z",
     "start_time": "2024-05-09T10:03:23.372853Z"
    }
   },
   "source": [
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "\n",
    "train, test = dataset_loader((160, 160), is_grayscale=False)\n",
    "\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train, batch_size=32, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test, batch_size=32, shuffle=True)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2 - Handmade Attempt\n",
    "\n",
    "## 2.1 - Model definition\n",
    "It is known that to approximate the non-linear problem the amount of hidden neurons of a NN has to between\n",
    "the input space and the output shape. For images of 160x160x3 in a binary classification it means the number of neurons required should be in the range: (2, 76800). Of course this is not feasible. We just try a structure hoping it to be rich enough to approximate well enough the objective function."
   ],
   "id": "3f59d1021ec84e20"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import keras\n",
    "from models.structure.base_model_wrapper import BaseModelWrapper\n",
    "\n",
    "class NaiveDnnWrapper(BaseModelWrapper):\n",
    "    def make_layers(self, input_shape: (int, int, int)) -> tuple[keras.Layer, keras.Layer]:\n",
    "        input_layer = keras.Input(shape=input_shape, name=self.__class__.__name__)\n",
    "        x = keras.layers.Flatten(data_format=self.data_format.name)(input_layer)\n",
    "\n",
    "        x = keras.layers.Dense(units=2048, activation=\"relu\")(x)\n",
    "        x = keras.layers.Dense(units=720, activation=\"relu\")(x)\n",
    "        output_layer = keras.layers.Dense(units=1, activation='sigmoid')(x)\n",
    "\n",
    "        return input_layer, output_layer"
   ],
   "id": "cd7f4147cf535102",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from models.structure.learning_parameters.sgd_learning_parameters import SgdLearningParameters\n",
    "from models.naive_dnn_gen.naive_dnn import NaiveDnnWrapper\n",
    "\n",
    "# For input of size 160x160x3 (150528) we have the problem to choose the neurons between that and 1 (output).\n",
    "# It would be reasonable to have large hidden layers. Yet we are limited.\n",
    "model = NaiveDnnWrapper().make_model((3, 160, 160))\n",
    "\n",
    "learning_parameters = SgdLearningParameters(learning_rate=1e-4)\n",
    "learning_parameters.compile_model(model)\n",
    "\n",
    "model.summary()"
   ],
   "id": "cefa795f3334cb96",
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "I don't know why but it considers 16 for batch size while I didn't explicit it.",
   "id": "33559fb682d66330"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T07:53:18.200288Z",
     "start_time": "2024-05-09T07:53:18.094493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "keras.utils.plot_model(\n",
    "    model, to_file='hand_tailored_conv_net.png', show_layer_names=True, expand_nested=True, show_shapes=True\n",
    ")"
   ],
   "id": "5f1d5432ec31b95a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:20:41.399260Z",
     "start_time": "2024-05-09T07:53:18.201161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "# In this simple approach we have fixed [Train 0.8 - Val 0.2 - Test] without doing any k-fold CV.\n",
    "# The number of epoches is fixed by early stopping. (In the case of k-fold-CV it will be a hp given by average performance?)\n",
    "history = model.fit(x=train_dataloader, batch_size=32, epochs=120, callbacks=[\n",
    "    # To persist the history\n",
    "    keras.callbacks.CSVLogger(f\"hand-tailored-dnn.log\", separator=\",\", append=True)\n",
    "])"
   ],
   "id": "569773d51ded19b7",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:21:17.863961Z",
     "start_time": "2024-05-09T08:21:01.349641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "persist_model: bool = True\n",
    "\n",
    "# We don't want to persist the model as I already saved it.\n",
    "if persist_model:\n",
    "    model.save('hand_tailored_dnn_net.keras')"
   ],
   "id": "49870fd41fc78e2a",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:21:21.140532Z",
     "start_time": "2024-05-09T08:21:21.138285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")\n",
    "print(f\"The model is not that bad considering we have no pre-processing done and the parameters were chosen freely\")"
   ],
   "id": "5c943ad153fa2dac",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:33:12.925869Z",
     "start_time": "2024-05-14T13:33:12.708837Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas\n",
    "\n",
    "csv = pandas.read_csv(f\"hand-tailored-dnn.log\")"
   ],
   "id": "3cf12415eb94d26a",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:22:20.484458Z",
     "start_time": "2024-05-09T08:22:20.270813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import plotly.express as px\n",
    "\n",
    "loss_figure = px.line(csv, x=\"epoch\", y=[\"loss\", \"val_loss\"], template=\"plotly_white\", markers=True)\n",
    "loss_figure.update_layout(title=\"Loss\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")"
   ],
   "id": "8b21900c81c3fef0",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-09T08:22:15.947285Z",
     "start_time": "2024-05-09T08:22:15.919696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "figure = px.line(csv, x=\"epoch\", y=[\"accuracy\", \"val_accuracy\"], template=\"plotly_white\", markers=True)\n",
    "figure.update_layout(title=\"Accuracy\", xaxis_title=\"Epoch\", yaxis_title=\"Loss\")"
   ],
   "id": "952e4de490c66fd2",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T13:33:14.725573Z",
     "start_time": "2024-05-14T13:33:14.298283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils.data_processing import make_loss_graphs, make_loss_accuracy_graphs\n",
    "\n",
    "loss_graph = make_loss_graphs(csv, False)\n",
    "loss_graph.update_layout(title=\"Loss vs Val_loss in tuner search per epoch (Val dashed)\")\n",
    "\n",
    "acc_graph = make_loss_accuracy_graphs(csv, False)\n",
    "acc_graph.update_layout(title=\"Accuracy vs Val_Accuracy in tuner search per epoch (Val dashed)\")\n",
    "\n",
    "loss_graph.show()\n",
    "acc_graph.show()"
   ],
   "id": "e5271e69025e2bfd",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The model isn't necessarily evil. It works fair enough not being tuned.\n",
    "Regularization and images pre-processing could lead to some good results while maybe it is the case to increase the parameters? All these considerations are to left to thought as the CNN (which is a state of the art approach) is only 1/3 of the current network and performs consistently better in the same amount of epochs on test while having more trouble fitting the training set.\n",
    "\n",
    "### CNN seem to be the path to take (as tweaking a DNN can be a really hard task)"
   ],
   "id": "1c5087a0fe9d576b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "44fee2d65b2a53d8",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
