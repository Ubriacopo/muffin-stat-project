{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Three layers tuning search.\n",
    "\n",
    "We want to learn the best possible architecture for a 3 hidden layers DNN.\n",
    "What the results show us is that, as we expect the training process to be longer and harder, the results are still not optimal great. The search space might be bad or simply, as we the input space suggests, the classification problem is too hard to be done on a such low scale of hidden units."
   ],
   "id": "8e1373649bb873c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:55:27.931491Z",
     "start_time": "2024-04-24T16:55:27.929687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Why Torch? You'll find the answer in the .md files! \n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\""
   ],
   "id": "5e7c92ba9c41ef74",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:55:30.160639Z",
     "start_time": "2024-04-24T16:55:28.753427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Dataset loading\n",
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "\n",
    "train, test = dataset_loader((128, 128), is_grayscale=False)\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=16, shuffle=True)"
   ],
   "id": "825d801da83e2b60",
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-04-24T16:55:31.248781Z",
     "start_time": "2024-04-24T16:55:30.600534Z"
    }
   },
   "source": [
    "import keras_tuner\n",
    "from utils.my_tuner import HistoryDeletingBayesianOptimization\n",
    "from models.naive_dnn_gen.naive_dnn import NaiveDnnTunableWrapper\n",
    "from models.structure.tunable_model_family_hypermodel import TunableModelFamilyHypermodel\n",
    "\n",
    "# Top results:\n",
    "# Reload the tuner.\n",
    "tuner = HistoryDeletingBayesianOptimization(\n",
    "    TunableModelFamilyHypermodel((3, 128, 128), NaiveDnnTunableWrapper()),\n",
    "    hyperparameters=keras_tuner.HyperParameters(), objective='val_loss',\n",
    "    tune_new_entries=True, overwrite=False, directory=\"dnn-search\",\n",
    "    max_trials=15, project_name=\"three-layers\"\n",
    ")\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:55:31.763726Z",
     "start_time": "2024-04-24T16:55:31.761202Z"
    }
   },
   "cell_type": "code",
   "source": "tuner.results_summary(5)",
   "id": "d01572cad808628a",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:55:42.307972Z",
     "start_time": "2024-04-24T16:55:42.295934Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas\n",
    "\n",
    "# Prepare the pandas dataframe so we can see what trial has which iterations done.\n",
    "csv = pandas.read_csv(\"./dnn-search/three-layers/search.log\")\n",
    "csv['tuner_iteration'] = 0\n",
    "\n",
    "current_iteration = 0\n",
    "for index, row in enumerate(csv.itertuples()):\n",
    "    csv.at[index, 'tuner_iteration'] = int(index / 22)\n",
    "\n",
    "best_dataframe = csv.query(\"tuner_iteration in [12]\")"
   ],
   "id": "a02e6d6167293d2",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:55:45.864851Z",
     "start_time": "2024-04-24T16:55:45.856991Z"
    }
   },
   "cell_type": "code",
   "source": "csv",
   "id": "ac6be3d76ce9ae92",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:32:03.624704Z",
     "start_time": "2024-04-24T16:32:03.617123Z"
    }
   },
   "cell_type": "code",
   "source": "best_dataframe",
   "id": "5bf383bc13c32717",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:47:24.635384Z",
     "start_time": "2024-04-24T16:47:24.528591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The best model was hardly overfitting. There are no dropout layers on the model which might make performance better. (Trial 12)\n",
    "# As the learning was steadily following the train accuracy we are probably missing units to better learn the structure.\n",
    "# We could try to tune it with different combinations of dropout layers and maybe few more neurons.\n",
    "best_dataframe[[\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"]].plot()\n",
    "# This model is of no use if we dont change some of its structure"
   ],
   "id": "7726283ee76b9b0f",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Best configuration with dropout",
   "id": "41d8ae773ee8dfe7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:37:08.164070Z",
     "start_time": "2024-04-24T16:37:08.159167Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Trial 14 had some dropout layers. Could it have been still learning well? Remember we have no early stopping.\n",
    "best_dataframe_with_dropout = csv.query(\"tuner_iteration in [14]\")"
   ],
   "id": "2fd8cf3b612fcc98",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:37:12.871848Z",
     "start_time": "2024-04-24T16:37:12.865538Z"
    }
   },
   "cell_type": "code",
   "source": "best_dataframe_with_dropout",
   "id": "6279827e434f9d17",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T16:46:49.628379Z",
     "start_time": "2024-04-24T16:46:49.529205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This model might be worth to be still trained without putting hand on the parameters.\n",
    "# We could still try to search for better parameters. (The neurons are prolly too few)\n",
    "best_dataframe_with_dropout[[\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"]].plot()"
   ],
   "id": "521dc50313ce8954",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:34:32.203246Z",
     "start_time": "2024-04-24T16:56:00.255679Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras.optimizers\n",
    "# Retrain the model with early stopping as it is. (K-fold will follow)\n",
    "dropout_best_family = NaiveDnnTunableWrapper()\n",
    "dropout_best_family.load_parameters(tuner.get_best_hyperparameters(5)[2])\n",
    "model = dropout_best_family.make_model((3, 128, 128))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(train_dataloader, validation_data=validation_dataloader, epochs=150, callbacks=[keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='min', restore_best_weights=True\n",
    ")])"
   ],
   "id": "ed96b806b36d1af",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:43:42.337418Z",
     "start_time": "2024-04-24T17:43:42.330953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframe = pandas.DataFrame(history.history)\n",
    "dataframe"
   ],
   "id": "9e6ade3bc6053899",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:45:18.814099Z",
     "start_time": "2024-04-24T17:45:18.715945Z"
    }
   },
   "cell_type": "code",
   "source": "dataframe[[\"loss\", \"val_loss\", \"accuracy\", \"val_accuracy\"]].plot()",
   "id": "f53bc809f498ae66",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:53:14.131666Z",
     "start_time": "2024-04-24T17:53:14.129336Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# The model doesnt look well. The gap in loss is noticable and the room for improvement was low. We could try using a pre-proecessing procedure on data to increase the sample size and also increase the neurons of the network.\n",
    "print(tuner.get_best_hyperparameters(5)[2].values)\n",
    "tuner.get_best_hyperparameters(5)[2].values['units_1'] = 1024\n",
    "tuner.get_best_hyperparameters(5)[2].values['dropout_1'] = True\n",
    "tuner.get_best_hyperparameters(5)[2].values['units_2'] = 248"
   ],
   "id": "cae5e1c554a39b7d",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T17:53:29.683919Z",
     "start_time": "2024-04-24T17:53:29.662933Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras.optimizers\n",
    "# Retrain the model with early stopping as it is. (K-fold will follow)\n",
    "dropout_best_family = NaiveDnnTunableWrapper()\n",
    "dropout_best_family.load_parameters(tuner.get_best_hyperparameters(5)[2])\n",
    "model = dropout_best_family.make_model((3, 128, 128))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "id": "6b3673558be524c8",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T18:37:38.325586Z",
     "start_time": "2024-04-24T17:53:32.741915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(train_dataloader, validation_data=validation_dataloader, epochs=150, callbacks=[keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-4, patience=10, verbose=1, mode='min', restore_best_weights=True\n",
    ")])"
   ],
   "id": "55761e643bde9358",
   "execution_count": 28,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:09:45.111605Z",
     "start_time": "2024-04-24T19:09:45.106524Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataframe = pandas.DataFrame(history.history)\n",
    "dataframe"
   ],
   "id": "67b8b73b50d7d479",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:20:07.672763Z",
     "start_time": "2024-04-24T19:20:07.535461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gc\n",
    "import torch\n",
    "\n",
    "# We learn slowly but we learn. We could try with augmentation. 80% on validation aint that bad.,\n",
    "# Yet the model before performs in a kinda similar way so where went the complexity of our model?\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ],
   "id": "84f60784c0b81524",
   "execution_count": 35,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T19:20:08.323864Z",
     "start_time": "2024-04-24T19:20:08.298467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Per sfizio lo rendiamo immenso\n",
    "print(tuner.get_best_hyperparameters(5)[2].values)\n",
    "tuner.get_best_hyperparameters(5)[2].values['units_0'] = 3072\n",
    "tuner.get_best_hyperparameters(5)[2].values['units_1'] = 1536\n",
    "tuner.get_best_hyperparameters(5)[2].values['dropout_1'] = True\n",
    "tuner.get_best_hyperparameters(5)[2].values['units_2'] = 256\n",
    "\n",
    "import keras.optimizers\n",
    "# Retrain the model with early stopping as it is. (K-fold will follow)\n",
    "dropout_best_family = NaiveDnnTunableWrapper()\n",
    "dropout_best_family.load_parameters(tuner.get_best_hyperparameters(5)[2])\n",
    "model = dropout_best_family.make_model((3, 128, 128))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=1e-4, momentum=0.9, nesterov=True), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ],
   "id": "a4967871464e97bc",
   "execution_count": 36,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-24T20:30:06.164455Z",
     "start_time": "2024-04-24T19:20:09.261103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "history = model.fit(train_dataloader, validation_data=validation_dataloader, epochs=150, callbacks=[keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=1e-4, patience=20, verbose=1, mode='min', restore_best_weights=True\n",
    ")])"
   ],
   "id": "41b85e061912eba3",
   "execution_count": 37,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "f8b4cb3a4a2f4ba3",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
