{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "label_mappings = {0: \"chihuahua\", 1: \"muffin\"}",
   "id": "5d1580063967358"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Case Studies we take a look at:\n",
    "- VGG-16 (from Keras models repository, fine tuned)\n",
    "- XCeption\n",
    "\n",
    "> https://www.topbots.com/important-cnn-architectures/"
   ],
   "id": "350b825d7a2597e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# https://keras.io/guides/transfer_learning/",
   "id": "f9f1cecb54733ab7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Utility functions\n",
   "id": "da752fb40c702b12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from keras.src import Functional\n",
    "from typing import Callable\n",
    "\n",
    "\n",
    "def make_evaluations_list(evaluate_model: Callable[[any, Functional], any], model: Functional,\n",
    "                          reference_dataset: ImageFolder, samples: int = 8) -> list[tuple[any, any, any]]:\n",
    "    \"\"\"\n",
    "    \n",
    "    :param evaluate_model: \n",
    "    :param model: \n",
    "    :param reference_dataset: \n",
    "    :param samples: \n",
    "    :return: A tuple containing: [the image ready for plotting, the predicted label from the model, the true label]\n",
    "    \"\"\"\n",
    "    for i in range(samples):\n",
    "        # Random draw\n",
    "        random_index = [int(i * len(test))]\n",
    "\n",
    "        img = reference_dataset[random_index][0]\n",
    "        label = reference_dataset[random_index][1]\n",
    "        yield torch.permute(img, (1, 2, 0)), evaluate_model(img, model), label\n",
    "\n",
    "\n",
    "def print_evaluation_lists_information(image_evaluations_list: list[tuple[any, any, any]]) -> None:\n",
    "    images = [i[0] for i in image_evaluations_list]\n",
    "\n",
    "    fig = px.imshow(np.array(images), binary_string=True, facet_col=0, facet_col_wrap=4)\n",
    "    fig.show()\n",
    "\n",
    "    for i in range(len(image_evaluations_list)):\n",
    "        _, prediction, correct_label = image_evaluations_list[i]\n",
    "        print(f\"For facet {i} model has predicted: {[prediction_entry[1] for prediction_entry in prediction]}. \"\n",
    "              f\"The correct label is {correct_label}\")"
   ],
   "id": "53a2ad69812d5c9c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1 - Xception\n",
    "Before training and fine tuning lets see the performance of the pretrained model on some samples."
   ],
   "id": "cd51aeb3cecd018"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:06:29.971236Z",
     "start_time": "2024-05-16T14:06:29.675856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import keras\n",
    "\n",
    "untouched_xception = keras.applications.Xception(weights='imagenet')"
   ],
   "id": "2e03e311896aa47a",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:06:31.065102Z",
     "start_time": "2024-05-16T14:06:31.062783Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_xception(image, xception, verbose: bool = False):\n",
    "    local_image = torch.permute(image, (1, 2, 0))\n",
    "    local_image = numpy.expand_dims(local_image, 0)\n",
    "\n",
    "    return keras.applications.xception.decode_predictions(xception.predict(local_image, verbose=verbose), top=3)[0]"
   ],
   "id": "2bb17d64ca75d580",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.2 - Data loading for Xception",
   "id": "4b06c416811c42ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "\n",
    "train, test = dataset_loader((299, 299), is_grayscale=False)\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test, batch_size=16, shuffle=True)"
   ],
   "id": "4814667f2fbebf16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1 -  Model Evaluation without training",
   "id": "3fe0e9ae9d0a6767"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m evaluations: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mtuple\u001B[39m] \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mrand(VISUALIZE_SAMPLES):\n\u001B[0;32m---> 13\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mtest\u001B[49m[\u001B[38;5;28mint\u001B[39m(i \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(test))][\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     14\u001B[0m     evaluations\u001B[38;5;241m.\u001B[39mappend((evaluate_xception(image, untouched_xception), test[\u001B[38;5;28mint\u001B[39m(i \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(test))][\u001B[38;5;241m1\u001B[39m]))\n\u001B[1;32m     15\u001B[0m     image_list\u001B[38;5;241m.\u001B[39mappend(torch\u001B[38;5;241m.\u001B[39mpermute(image, (\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m0\u001B[39m)))\n",
      "\u001B[0;31mNameError\u001B[0m: name 'test' is not defined"
     ]
    }
   ],
   "execution_count": 1,
   "source": "print_evaluation_lists_information(make_evaluations_list(evaluate_xception, untouched_xception, train_dataloader))",
   "id": "4786b908a5c73d33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# The model does not recognize Muffins as being muffins just as bakery (Which makes sense)\n",
    "# As the label Muffin is missing in the decoding of the Xception we just map it to bakery"
   ],
   "id": "3b61df463b12a6e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "predictions: list[tuple[list, int]] = [(evaluate_xception(i[0], untouched_xception), i[1]) for i in test]",
   "id": "50e019d48ef72cd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Redefine the labels to fit the observations on Xception\n",
    "label_mappings = {0: [\"chihuahua\", \"dog\"], 1: [\"muffin\", \"bakery\"]}\n",
    "\n",
    "TP = 0  # True positives\n",
    "for i in range(len(predictions)):\n",
    "    predicted_values = [j[1].lower() for j in predictions[i][0]]\n",
    "    true_label = label_mappings[predictions[i][1]]\n",
    "\n",
    "    TP += 1 if set(predicted_values) & set(true_label) else 0\n",
    "\n",
    "precision = TP / len(predictions)\n",
    "precision  # On the top 3 considering the fact that many miss classifications happen for the fact that the labels are more accurate for the dogs (some samples in the training set are not chihuahuas) and that Muffins do not have a real label."
   ],
   "id": "93359fabc60beab8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1.2 - Fine tuning the model\n",
    "We follow the following guide: https://keras.io/guides/transfer_learning/"
   ],
   "id": "7762e48f7089294d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2.1 - Model definition",
   "id": "69add1ea9d110677"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:16:05.624054Z",
     "start_time": "2024-05-16T14:16:05.621105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from models.structure.base_model_wrapper import BaseModelWrapper\n",
    "from models.structure.augmentation_wrapper import InvertedChannelsAugmentationWrapper\n",
    "import keras\n",
    "\n",
    "\n",
    "# https://keras.io/guides/transfer_learning/#the-typical-transferlearning-workflow \n",
    "# With augmentation just to permute here\n",
    "class XceptionAugmented(BaseModelWrapper):\n",
    "    latest_xception_model: Functional\n",
    "\n",
    "    def make_layers(self, input_shape: (int, int, int)) -> tuple[keras.Layer, keras.Layer]:\n",
    "        C, W, H = input_shape\n",
    "\n",
    "        inputs = keras.Input(input_shape)\n",
    "\n",
    "        x = keras.layers.Permute((2, 3, 1))(inputs)\n",
    "        x = keras.layers.Rescaling(scale=1 / 2, offset=-1)(x)\n",
    "\n",
    "        self.latest_xception_model = keras.applications.Xception(\n",
    "            weights='imagenet', include_top=False, input_shape=(W, H, C)\n",
    "        )\n",
    "\n",
    "        self.latest_xception_model.trainable = False\n",
    "        x = self.latest_xception_model(x, training=False)\n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        return inputs, outputs"
   ],
   "id": "7c00a10b3e288eb7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2.2 - Training and evaluation only on appended structure",
   "id": "e35e65973289a7f1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:16:06.569601Z",
     "start_time": "2024-05-16T14:16:06.254369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "xception_aug = XceptionAugmented()\n",
    "model = xception_aug.make_model((3, 299, 299))\n",
    "# Default configuration for the Xception model learning\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "id": "32bc7db2127080f1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001B[1mModel: \"functional_3\"\u001B[0m\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001B[1m \u001B[0m\u001B[1mLayer (type)                   \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mOutput Shape          \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m      Param #\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (\u001B[38;5;33mInputLayer\u001B[0m)      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m3\u001B[0m, \u001B[38;5;34m299\u001B[0m, \u001B[38;5;34m299\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ permute_1 (\u001B[38;5;33mPermute\u001B[0m)             │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m299\u001B[0m, \u001B[38;5;34m299\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (\u001B[38;5;33mRescaling\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m299\u001B[0m, \u001B[38;5;34m299\u001B[0m, \u001B[38;5;34m3\u001B[0m)    │             \u001B[38;5;34m0\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ xception (\u001B[38;5;33mFunctional\u001B[0m)           │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m10\u001B[0m, \u001B[38;5;34m2048\u001B[0m)   │    \u001B[38;5;34m20,861,480\u001B[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m2048\u001B[0m)           │             \u001B[38;5;34m0\u001B[0m │\n",
       "│ (\u001B[38;5;33mGlobalAveragePooling2D\u001B[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001B[38;5;33mDense\u001B[0m)                 │ (\u001B[38;5;45mNone\u001B[0m, \u001B[38;5;34m1\u001B[0m)              │         \u001B[38;5;34m2,049\u001B[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ permute_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Permute</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ rescaling (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Rescaling</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">299</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ xception (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Total params: \u001B[0m\u001B[38;5;34m20,863,529\u001B[0m (79.59 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,863,529</span> (79.59 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Trainable params: \u001B[0m\u001B[38;5;34m2,049\u001B[0m (8.00 KB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> (8.00 KB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001B[1m Non-trainable params: \u001B[0m\u001B[38;5;34m20,861,480\u001B[0m (79.58 MB)\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,861,480</span> (79.58 MB)\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:36:20.540728Z",
     "start_time": "2024-05-16T14:16:08.997194Z"
    }
   },
   "cell_type": "code",
   "source": "history = model.fit(train_dataloader, epochs=20, validation_data=validation_dataloader)",
   "id": "d2ea8f80435452a9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m62s\u001B[0m 264ms/step - binary_accuracy: 0.5680 - loss: 0.6803 - val_binary_accuracy: 0.6389 - val_loss: 0.6509\n",
      "Epoch 2/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 254ms/step - binary_accuracy: 0.6910 - loss: 0.6361 - val_binary_accuracy: 0.7603 - val_loss: 0.6074\n",
      "Epoch 3/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 255ms/step - binary_accuracy: 0.8086 - loss: 0.5901 - val_binary_accuracy: 0.8310 - val_loss: 0.5678\n",
      "Epoch 4/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 254ms/step - binary_accuracy: 0.8623 - loss: 0.5559 - val_binary_accuracy: 0.8817 - val_loss: 0.5319\n",
      "Epoch 5/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 254ms/step - binary_accuracy: 0.8947 - loss: 0.5255 - val_binary_accuracy: 0.9134 - val_loss: 0.5023\n",
      "Epoch 6/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 254ms/step - binary_accuracy: 0.9187 - loss: 0.4946 - val_binary_accuracy: 0.9282 - val_loss: 0.4766\n",
      "Epoch 7/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 254ms/step - binary_accuracy: 0.9343 - loss: 0.4662 - val_binary_accuracy: 0.9419 - val_loss: 0.4520\n",
      "Epoch 8/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 255ms/step - binary_accuracy: 0.9426 - loss: 0.4439 - val_binary_accuracy: 0.9504 - val_loss: 0.4268\n",
      "Epoch 9/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 257ms/step - binary_accuracy: 0.9448 - loss: 0.4237 - val_binary_accuracy: 0.9599 - val_loss: 0.4078\n",
      "Epoch 10/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 255ms/step - binary_accuracy: 0.9594 - loss: 0.4024 - val_binary_accuracy: 0.9630 - val_loss: 0.3909\n",
      "Epoch 11/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 255ms/step - binary_accuracy: 0.9553 - loss: 0.3880 - val_binary_accuracy: 0.9673 - val_loss: 0.3737\n",
      "Epoch 12/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 254ms/step - binary_accuracy: 0.9635 - loss: 0.3654 - val_binary_accuracy: 0.9683 - val_loss: 0.3610\n",
      "Epoch 13/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 253ms/step - binary_accuracy: 0.9622 - loss: 0.3621 - val_binary_accuracy: 0.9704 - val_loss: 0.3457\n",
      "Epoch 14/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 256ms/step - binary_accuracy: 0.9625 - loss: 0.3442 - val_binary_accuracy: 0.9725 - val_loss: 0.3325\n",
      "Epoch 15/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 256ms/step - binary_accuracy: 0.9663 - loss: 0.3371 - val_binary_accuracy: 0.9725 - val_loss: 0.3226\n",
      "Epoch 16/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m60s\u001B[0m 255ms/step - binary_accuracy: 0.9681 - loss: 0.3219 - val_binary_accuracy: 0.9725 - val_loss: 0.3107\n",
      "Epoch 17/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 257ms/step - binary_accuracy: 0.9736 - loss: 0.3078 - val_binary_accuracy: 0.9725 - val_loss: 0.3026\n",
      "Epoch 18/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 257ms/step - binary_accuracy: 0.9703 - loss: 0.3011 - val_binary_accuracy: 0.9725 - val_loss: 0.2934\n",
      "Epoch 19/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 259ms/step - binary_accuracy: 0.9742 - loss: 0.2884 - val_binary_accuracy: 0.9736 - val_loss: 0.2841\n",
      "Epoch 20/20\n",
      "\u001B[1m237/237\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m61s\u001B[0m 260ms/step - binary_accuracy: 0.9763 - loss: 0.2815 - val_binary_accuracy: 0.9747 - val_loss: 0.2758\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-16T14:46:15.796208Z",
     "start_time": "2024-05-16T14:45:59.108681Z"
    }
   },
   "cell_type": "code",
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")"
   ],
   "id": "816db91bd9e94df2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m74/74\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m17s\u001B[0m 224ms/step - binary_accuracy: 0.9583 - loss: 0.2803\n",
      "Test accuracy is 95.86% while loss is 0.28337985277175903\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2.3 - Free the network and final training ",
   "id": "deadaab7dcbaf477"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "xception_aug.latest_xception_model.trainable = True\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_dataloader, epochs=10, validation_data=validation_dataloader)"
   ],
   "id": "7d5e13a6d2c9c349"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.3 - K Fold Cross Validation",
   "id": "fa6d1bbe764c067f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c2a2098494338479"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2- VGG-16\n",
    "https://arxiv.org/abs/1409.1556"
   ],
   "id": "742da383b8529833"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 - Data loading for VGG-16",
   "id": "5672622774b4650d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "\n",
    "train, test = dataset_loader((224, 224), is_grayscale=False)\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test, batch_size=16, shuffle=True)"
   ],
   "id": "174568df8984bf9e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.1 -Model evaulation without training",
   "id": "f6984c686d18a7fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "untouched_vgg16 = keras.applications.VGG16(weights='imagenet')",
   "id": "8849a8ce2c3639b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate_vgg16(image, vgg16, verbose: bool = False):\n",
    "    local_image = torch.permute(image, (1, 2, 0))\n",
    "    local_image = numpy.expand_dims(local_image, 0)\n",
    "\n",
    "    return keras.applications.vgg16.decode_predictions(vgg16.predict(local_image, verbose=verbose), top=3)[0]"
   ],
   "id": "261473ad88fd5be1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# todo make this a method. We repeat it 3 times like idiots\n",
    "# We look at 8 samples directly\n",
    "VISUALIZE_SAMPLES: int = 8\n",
    "image_list: list = []\n",
    "\n",
    "evaluations: list[tuple] = []\n",
    "\n",
    "for i in torch.rand(VISUALIZE_SAMPLES):\n",
    "    image = test[int(i * len(test))][0]\n",
    "    evaluations.append((evaluate_vgg16(image, untouched_vgg16), test[int(i * len(test))][1]))\n",
    "    image_list.append(torch.permute(image, (1, 2, 0)))\n",
    "\n",
    "fig = px.imshow(np.array(image_list), binary_string=True, facet_col=0, facet_col_wrap=4)\n",
    "fig.show()\n",
    "\n",
    "for i in range(len(evaluations)):\n",
    "    print(\n",
    "        f\"For facet {i} VGG16 has prediceted: {[i[1] for i in evaluations[i][0]]} while the true label is {label_mappings[evaluations[i][1]]}\")"
   ],
   "id": "4ec17999abb7e0f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import keras\n",
    "\n",
    "predictions: list[tuple[list, int]] = [(evaluate_vgg16(i[0], untouched_vgg16), i[1]) for i in test]"
   ],
   "id": "a39c84388187ef30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "label_mappings = {0: [\"chihuahua\", \"dog\"], 1: [\"muffin\", \"bakery\"]}\n",
    "TP = 0  # True positives\n",
    "for i in range(len(predictions)):\n",
    "    predicted_values = [j[1].lower() for j in predictions[i][0]]\n",
    "    true_label = label_mappings[predictions[i][1]]\n",
    "\n",
    "    TP += 1 if set(predicted_values) & set(true_label) else 0\n",
    "\n",
    "precision = TP / len(predictions)\n",
    "precision  # On the top 3 considering the fact that many miss classifications happen for the fact that the labels are more accurate for the dogs (some samples in the training set are not chihuahuas) and that Muffins do not have a real label."
   ],
   "id": "394145f5f0d68c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.2 - Fine tuning the model",
   "id": "35ad05782b3e03a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2.1 -  Model definition",
   "id": "9529bae626cd067d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from models.structure.base_model_wrapper import BaseModelWrapper\n",
    "from models.structure.augmentation_wrapper import InvertedChannelsAugmentationWrapper\n",
    "import keras\n",
    "\n",
    "\n",
    "# https://keras.io/guides/transfer_learning/#the-typical-transferlearning-workflow \n",
    "# With augmentation just to permute here\n",
    "class VGG16Custom(BaseModelWrapper):\n",
    "    latest_model: Functional\n",
    "\n",
    "    def make_layers(self, input_shape: (int, int, int)) -> tuple[keras.Layer, keras.Layer]:\n",
    "        C, W, H = input_shape\n",
    "        inputs = keras.Input(input_shape)\n",
    "\n",
    "        x = keras.applications.mobilenet.preprocess_input(inputs, data_format=self.data_format.value)\n",
    "        x = keras.layers.Permute((2, 3, 1))(x)\n",
    "\n",
    "        self.latest_model = keras.applications.VGG16(\n",
    "            weights='imagenet', include_top=False, input_shape=(W, H, C)\n",
    "        )\n",
    "\n",
    "        self.latest_model.trainable = False\n",
    "        x = self.latest_model(x, training=False)\n",
    "        x = keras.layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "        outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n",
    "        return inputs, outputs"
   ],
   "id": "42685059a78477c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2.2. - Training and evaluation on frozen base model",
   "id": "db707d243a1fd6df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vgg_custom_wrapper = VGG16Custom()\n",
    "model = vgg_custom_wrapper.make_model((3, 224, 224))\n",
    "# Default configuration for the Xception model learning\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "id": "c996bdc7f09c444d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "history = model.fit(train_dataloader, epochs=20, validation_data=validation_dataloader, callbacks=[\n",
    "    \n",
    "])"
   ],
   "id": "7c41a93b0525dcc6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "res = model.evaluate(test_dataloader)\n",
    "print(f\"Test accuracy is {res[1] * 100:.2f}% while loss is {res[0]}\")"
   ],
   "id": "7a936ac5fcf5d577"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2.3 - Fine tuning on the whole structure ",
   "id": "4e9dbc7dfc8227b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vgg_custom_wrapper.latest_model.trainable = True\n",
    "model.compile(optimizer=keras.optimizers.Adam(1e-5), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(train_dataloader, epochs=10, validation_data=validation_dataloader)"
   ],
   "id": "5ac80eb580755204"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2.3 - K fold CV",
   "id": "527d975a5203503a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "k_fold_controller = dataset_split_controller\n",
    "results = []\n",
    "for i in range(k_fold_controller.k):\n",
    "    # Delete previous model in memory todo\n",
    "    \n",
    "    local_train, local_test = dataset_split_controller.get_data_for_fold(i)\n",
    "    \n",
    "    train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "    test_dataloader = DataLoader(dataset=local_test, batch_size=16, shuffle=True)\n",
    "    \n",
    "    # Do we want to early stop? If so we need to split train further to have a validation split.\n",
    "    model = vgg_custom_wrapper.make_model((3, 224, 224))\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    \n",
    "    model.fit(train_dataloader, epochs=20, callbacks=[])\n",
    "    results.append(model.evaluate(test_dataloader))"
   ],
   "id": "e0160be98d01fa2d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset.k_fold_dataset_wrapper import KFoldDatasetWrapper\n",
    "\n",
    "from dataset.dataset_loader import dataset_loader\n",
    "\n",
    "train, test = dataset_loader((224, 224), is_grayscale=False)\n",
    "dataset_split_controller = KFoldDatasetWrapper(5)\n",
    "dataset_split_controller.load_data(train)\n",
    "\n",
    "local_train, validation = dataset_split_controller.get_data_for_fold(0)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=local_train, batch_size=16, shuffle=True)\n",
    "validation_dataloader = DataLoader(dataset=validation, batch_size=16, shuffle=True)\n",
    "test_dataloader = DataLoader(dataset=test, batch_size=16, shuffle=True)"
   ],
   "id": "9e7cbdd07e718d1b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
