\newpage


\section{Hyperparameters Space}
\label{sec:hyperparameters-space}

Before we describe the models we have to recognize what defines our learning algorithm in its entirety.
The parameters that influence the learning process and are defined beforehand are the so-called \textbf{Hyperparameters}.

% todo lo scrivo?
For the no-free-lunch theorem it is know that there is no correct setting that works best for any situation meaning
that these parameters, while often are good enough in some \"default\" ranges, are domain specific.
It's therefore common practice to use some heuristics and to later tune these parameters to build a better model.

In the context of Neural Networks the hyperparameters skyrocket as the entire structure of the network
is a set of hyperparameters itself. Some parameters on which we should like to shine some light on are:

\paragraph{Image size and channels}
The size of the input is an important element when designing a prediction model as we face two possible problems:
\begin{itemize}
    \item Disomogenea images makes it almost impossible to build a network unless we work with some techniques like
    zero padding (meaning all images are all enlarged to the same size, leading to redundant data) or the use of RNN and embedding. %todo vedi se si scrive cosi
    \item Large input size can lead to an exploding number of parameters for the network to fit.
\end{itemize}

Downscaling an image has also the advantage of acting as a regularization technique as the images we are providing
show less detail and enhance recognizable features.
This means that most of the time, if the image size is not reduced too much, the model generalizes better.

In our case images are not in grayscale, and we kept it like this.
The choice of the size comes down to the problem and, as we want a more memory restricted model in the case of the MLP,
we opted for a common image size used in CNNs: 3x224x224 as it has been adopted by many solid models and is a good size to start with.
% todo reference. Ho questa; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8700246/ ma Ã¨ legata a endescopy

\paragraph{Batch size}



\paragraph{Loss function (Objective)}
crossentropyloss
shortly what is a loss function
what it measures

why cross entropy and not hinge


\paragraph{Optimizer}
An optimizer is a method that minimizes, therefore optimizes, a given objective function (loss function).
The problem of minimization is the following when speaking of optimizers:

$$ Q(w) = 1/n \sum{}$$

what an optimizer is
sgd, adam (main comparison is it)
paper showing sgd converges better at cost of time
less parameters to tune (reference to chapter 3)
less memory imprint (4n with momentum)


\paragraph{Epochs}


\paragraph{Activation Function}
relu and not sigmoid except for output
-> relu avoids problems with gradients (tanh was also a solid choice but nowdays relu is more popular)

\paragraph{Neurons}

\paragraph{Layer type}

\paragraph{Free hyperparameters}
we have some free
-> this is wanted

bayesian optimization instead of nested k-fold CV (keras tuner)

\paragraph{Neural Networks Hyperparameters}

neurons
convolution layers information