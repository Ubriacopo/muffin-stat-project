\newpage


\section{Hyperparameters Space}
\label{sec:hyperparameters-space}
Many machine learning algorithms are not strictly algorithms as they have some parameters to
set before the training process can start, called \textit{Hyperparameters}.
Those are considered families of learning algorithms of which Neural Networks are part of.
They are defined as  $\{A_\theta : \theta \in \Theta\}$ where $\theta$ is an instance of the possible hyperparameters space $\Theta$.

Due to the \textit{no-free lunch theorem} there is no general setting that works best for any situation.
This means that the hyperparameters, while often good in enough in some "default" ranges, are specific to the learning problem.
It's common practice to apply some heuristics on these parameters and to later tune them in order to build a better model.


\paragraph{}
Some parameters on which we should like to shine some light on are:

\subsection{Image size and channels}
\label{subsec:image-size-and-channels}
The size of the input is an important element when designing a prediction model as we face two possible problems:
\begin{itemize}
    \item Images of different sizes require special techniques to be elaborated
    \item Large input size can lead to a high number of parameters
\end{itemize}

Downscaling an image has also the advantage of acting as a regularization technique as the images we are providing
show less detail and enhance recognizable features.
This means that in most cases, if the image size is not reduced too much, the model tends to generalize better.

In the development of the project images were kept RGB and resized to $(224 \times 224)$ as it has been adopted by many
solid models, like \textit{VGG-16}(\ref{subsubsec:vgg16}).
% todo continua da qui

\subsection{Loss function}
\label{subsec:loss-function-(objective)}
The loss function $(l)$ is used to measures the difference between the predicted value $\hat{y}$ and the real value $y$.
In statistical learning it is part of the specification of a learning problem $(D,l)$ where $D$ is the data distribution.
As $D$ is unknown the test set is created to measure the error
of the predictor that is with a good probability, by the \textit{Chernoff-Hoeffding bound}, close to the statistical risk.\cite{} %todo cite

For the project the \textbf{Binary Cross Entropy loss} was used:
\[l(y, \hat{y}) = - [y\log(\hat{y}) + (1 - y)\log(1 - \hat{y})]\]

It is the negative log-likelihood which measures the probability of observing
the data assuming specific values for the parameters.

The loss function, compared to alternatives like the \textbf{hinge loss}, has the advantage of giving a probabilistic
interpretation of the model outputs while also giving a harsher penalization to miss-classifications.

It was requested to run \textit{k-fold CV} to calculate the model's risk estimates and report them according to the values of the \textbf{0-1 loss}:
$l(y,\hat{y}) = \mathds{1}(y \neq \hat{y})$.

\subsection{Optimizer}
\label{subsec:optimizer}
It is an algorithm that optimizes a given objective function.

Most optimizers are gradients based with \textbf{Stochastic Gradient Descent}(SGD) being one of the first, and still relevant today.
An step for SGD is defined as: \[w \coloneqq w - \eta \nabla J(w) \].

For the project SGD was the to go to as it still isn't outclassed by other methods\cite{wilson2018marginal} and also
has few advantages compared to other optimizers such as having a lower memory footprint and fewer hyperparameters\cite{optimzierChoice} to tune.

The default learning rate ($\eta$) is set to 0.01 but later fine-tuned.

\subsection{Batch size}
\label{subsec:batch-size}
While the optimizer choice fell for SGD we actually run a variant of it being the \textbf{mini-batched} version.
This version simply takes a small subset of the training data and averages the calculated gradients before updating the weights of the model.
This usually leads to a faster convergence and also better results in general while having smaller memory requirements.

Another thing to consider is that, in practice, it was observed that lager batches tend to degrade the model's quality\cite{keskar2017largebatch}

The initial batch size was set to 32 as it is a recommended starting value.

\subsection{Epochs}
\label{subsec:epochs}
An epoch is a training step computed on the full training set.
Based on the machine learning algorithm a single iteration might be sufficient, as for k-NN, but that's not the
case for NNs as they use gradient based optimization techniques.

The initial number of epochs has been set to 15 and later replaced by the flexible \textbf{Early Stopping} regularization technique.

The error on the validation set is used as a proxy to estimate the generalization error and, if there hasn't been
any improvement for a set amount of epochs (\textit{patience}=5) the training procedure is interrupted.

In \textit{keras} this is obtained by increasing the maximum number of epochs and adding to the \textit{fit} procedure the callback:
\begin{verbatim}
    keras.callbacks.EarlyStopping(
            monitor='val_loss', patience=5, verbose=1, mode='min'
    )
\end{verbatim}

By default, once finished, the best network weights are restored.

\subsection{Hyperparameter Tuning}\label{subsec:hyperparameter-tuning}
While having fixed hyperparameters can lead to satisfying results it is desirable to have the best possible configuration
of the learning process for the model.
The procedure of learning hyperparameters can be obtained in various ways, such as nested \textbf{k-fold CV}.
In the project we opted for the exposed \textit{Bayesian Optimization Tuner} that is provided by \textit{KerasTuner}.
It works iteratively by placing priors on the unknown function and to update them in the form of posterior distribution over the target.
The distribution is then used to construct an acquisition function that determines the next query point and the process is repeated.
The higher the amount of observations the more confident the algorithm becomes of certain regions in space.\cite{snoek2012practical}

The tuning process was applied in two ways, as described below.

\paragraph{Tuning the NN structure}
In the context of Neural Networks the hyperparameters skyrocket as the entire structure of the network is a set of hyperparameters itself.
Therefore, we wanted to learn a possibly better structure than the ones defined by trial and error.
This is shown in depth in the section \ref{subsubsec:autotuned}.

\paragraph{Tuning the learning hyperparameters}
In order to improve the learning process for a specific model we learn the following list of hyperparameters on it:
\begin{itemize}
    \item Learning rate: set between [1e-5, 1e-2] with a step of 2 on log sampling.
    \item Batch size:  $\in \{8, 16, 32, 64\}$ to avoid too large values.
    \item Momentum (of SGD): set between [0.5, 1] with a linear step of 0.05
\end{itemize}