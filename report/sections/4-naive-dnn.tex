\newpage
\section{Naive MLP}
\label{sec:naive-mlp}

The first network structure we tailored on is the classic \textbf{Multilayer Perceptron}(MLP), not to be confused with
the first proposed Perceptron by McCulloch and Pitts\cite{mcculloch43a} which does share some similarities with it.

These networks are feedforward, meaning they propagate information only to the following layers at evaluation time.

Each layer of the neural network of this structure is made of units, also called neurons, that connect
to all the ones of the following layer, thus they are called \textbf{fully connected}.
In each neuron the inputs are weighted by a matrix of coefficients, weights ($w$), summed and passed to an activation function($\sigma$):

\[g(x) = \sigma(w^Tx)\]

The activation function is usually non-linear. The computed value is what is passed as an input to all the neurons of the next layer.
The parameters that the predictor has to learn during the training process are the values of the weight matrix.
Of course the values of the matrix will be zero ($w_{i,j} = 0$) when $(i,j) \notin E$ where $E$ are the edges of the acyclic graph
that connect two neurons, as such connection does not exist.

To implement a simple MLP using Keras only two layers types are needed:
\begin{verbatim}
# This defines an input object
x = keras.layers.Input(shape=(32,))
# This defines a fully connected layer that takes the previous one in input.
x = keras.layers.Dense(128, activation="relu")(x)
\end{verbatim}

All feed-forward neural network models are trained using the \textbf{backpropagation} algorithm\cite{kelley1960gradient}.

\subsection{Proposed model}
Two proposals were made for the creation of a MLP.

\paragraph{First Model}
A simple fully connected network that \textit{Flattens} the input (3x224x224) and is followed by two hidden layers
of 1024 and 256 units respectively. At top of the model there is a single unit layer with sigmoid activation acting as classifier.

\paragraph{Second Model}
Instead of having two hidden layers this model has only one of 128 units while being structurally identical to the first model.

\subsection{Observed Results}
% grafi andamneti dei due modelli + modello finale test valutazione

Both models overfit. The second one, which is way smaller than the first one, still has too many parameters and leads to overfitting.
Regularization techniques could be applied to increase the performance of the network but, considering CNN are both mostly
better and easier to implement when working with images, we shift our focus towards CNN's and leave DNNs behind.
% todo stampa dati?
